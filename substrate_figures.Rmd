---
title: "Substrate Tables and Figures"
author: "Edward Gregr"
date: "2020/11/20"
output: word_document
---

See Plot.Obs.By.IDS.For.Regions for facet strip formatting stuff.   

Tables 1 and 2 are not generated by this script. They need to be copied from the working version of this file. See the document assembly notes in the supporting tech document. 

Outstanding code revisions:
- Create summary tables to populate relevant tables
- Decide on IDE class-based figure. 
- Sort out why this script occassionally creastes an empty folder.

Outstanding text revisions:
- Finish populating Tables 1 and S2; add refs to Table S3.
- 


```{r Build_Tables, echo=FALSE}
library( knitr )
knitr::opts_chunk$set(fig.width=12, fig.height=8) 

#-- Main statistics don't make a very nice bar plot. Report as table. 
T.cap <- "Table 3: Aggregated model build metrics for all models, comparing the weighted (first row) and the unweighted (second row) random forest results. Out of bag (OOB) values show the mean prediction error from the random forest internal re-sampling. The true skill statistic (TSS) corrects Accuracy for chance and prevalence expressing how model performance exceeds random, while Accuracy, Specificity, Exchange and Shift provide an assessment of model error. See Supplementary materials S1 for more details on the metrics." 

x <- rbind( build.sum$build.results.Integrated,
            build.sum.nw$build.results.Integrated )

# adjust region levels ... arguably should be much earlier ... 
# levels( x$Region ) <- fct_relevel(x$Region, "Coast", "HG", "NCC", "WCVI", "QCS", "SOG" )

x <- x[ order(x$Region),]
x <- x[ , c( 'Region', 'N', 'Imbalance', 'OOB', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift')]
row.names(x) <- NULL

kable( x , digits=2, caption = T.cap )


# Create table of how 100 m coastal model predicts at regional scale. 
# REQUIRES test.regions to exist. 
T.cap <- "Table 4: Aggregated metrics for the 100 m model built for each region (see Table 1 for a description of the metrics). Regional sample sizes differ slightly from Table 1 because for this comparison the build data were separated using a regional shape file, whereas in Table 1 the points were selected to region using regional 20 m rasters." 

x <- rbind( 
    cbind( 'Region' = 'Coast', Results.Row( rf.region.Coast, test.regions$Coast )$Integrated), 
    cbind( 'Region' = 'HG',    Results.Row( rf.region.Coast,    test.regions$HG )$Integrated), 
    cbind( 'Region' = 'NCC',   Results.Row( rf.region.Coast,   test.regions$NCC )$Integrated),
    cbind( 'Region' = 'WCVI',  Results.Row( rf.region.Coast,  test.regions$WCVI )$Integrated),
    cbind( 'Region' = 'QCS',   Results.Row( rf.region.Coast,   test.regions$QCS )$Integrated),
    cbind( 'Region' = 'SOG',   Results.Row( rf.region.Coast,   test.regions$SOG )$Integrated)
  )

x <- x[ , c( 'Region', 'N', 'Imbalance', 'OOB', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift')]
row.names( x ) <- NULL

kable( x, digits=2, caption = T.cap )


# Table of IDE results. 

T.cap <- "Table 5: Performance of each random forest model against each independent data set." 

#- Build an IDE results table for the Wtd model ... 

x <- IDE.results.wtd$Integrated
y <- rbind( x[x$IDS=='Dive',], 
            x[x$IDS=='Cam',],
            x[x$IDS=='ROV',]
  )
z <- cbind( 'IDS'=y$IDS, y[,-2] )
rownames( z ) <- NULL

x <- z
x <- x[, !(names(x) %in% c("OOB")) ]
x <- x[ , c( 'IDS', 'Region', 'N', 'Imbalance', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift')]
#names(x)[7] <- 'Specificity'
row.names( x ) <- NULL

kable( x, digits=2, caption = T.cap )


# Build ANOVA from depth.results argument ... 

# # Copied from IDE_Main.R ... 
# names(depth.results)
# cor( depth.results[, -c(1:3)] )
# 
# # replace TSS w other metrics to create results. 
# a <- lm( TSS ~ Model + Region + Ribbon + N + Imbalance, data = depth.results )
# summary(a)
# anova( a )

```   

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig_1_Boundaries, echo=FALSE}

#-- Insert pre-built boundary file as png.
```

**Figure 1: Map of study area showing outline of the shelf (for the 100 m coastwide model) and the five regions including Haida Gwaii (HG), North Central Coast (NCC), West Coast of Vancouver Island (WCVI), Queen Charlotte Strait (QCS), and the Strait of Georgia (SOG).**

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig_2_Build_Heatmaps, echo=FALSE}

#--- Heatmaps (tigures!) of build stats. 
# As superheat library only makes pngs, they have to be pre-built and then just called here. 

# 2020/09/04: Now pre-built multi-panel from the heatmap output.

```
**Figure 2: Heatmaps of class-based Accuracy, true negative rate (TNR) and Reliability for weighted (left column) and non-weighted (right column) models. The colour shading within each row reflects the underlying numbers from high (red) to low (blue) and is included to make the differences between values more apparent.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig_3_Build_Prevalence, echo=FALSE }

#-- Prevalence plots (obs & predicted) for build (testing partition), faceted by Region
a <- build.sum$build.results.ClassPrev
b <- build.sum.nw$build.results.ClassPrev[7:12,]

# change numbers to proportions
x <- a[, names(a) %in% c('Hard', 'Mixed', 'Sand', 'Mud')]
y <- data.frame(  x / rowSums( x ))

# add results back to the Region column
z <- cbind( 'Region' = a$Region, 'Source' = c(rep('Observed',6), rep('Weighted',6)), y)

#--- do it again with the non-wtd model, this time keep only the predictions ... 

x <- b[, names(b) %in% c('Hard', 'Mixed', 'Sand', 'Mud')]
y <- data.frame(  x / rowSums( x ))
zz <- cbind( 'Region' = b$Region, 'Source' = rep('No Weights',6), y)

foo <- rbind( z, zz)

Plot.Obs.Pred.Prevalence.Build( foo, pal.cb3b, sz=30, lx=0.15, ly=0.85 ) 

# save plot to 'a' if using ggsave()
# ggsave("Class Prevalence for Obs&Pred (Build) for Regions.png", a, dpi = 300, width = 16, height = 10, path = results.dir)

```

**Figure 3: Observed class prevalences in the build testing partition (orange) compared to predictions from the weighted (yellow) and unweighted (blue) random forest models across models.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/figures/ms_figure4_RegionalMaps.png){ fig.fullwidth=TRUE }

**Figure 4: Predictions from three different models (20 m weighted – top row; 20 m unweighted – middle row, and 100 m weighted – bottom row) for the Pacific Rim (left column) and Greater Vancouver (right column) assessment areas.**

Will want to address the blue spot in row 1 caused by the clipped bathymetry.

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/heat_VariableImportance_Build.png){ fig.fullwidth=TRUE }

**Figure 5: Heatmap of variable importance across models. Importance is defined as the proportion of each predictor's contribution to the model, relative to the predictor with the highest contribution. (p/max(p)). The colour shading within each row reflects the underlying numbers from high (red) to low (blue) and is included to make the differences in the values more apparent. NA indicates that Fetch was not used as a predictor for the Coast model.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig6a_Depth_Results_by_Resolution_TSS, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}


# This code  builds the data used by the panels in Fig 7.
z <- depth.results

# Build some difference columns, and add them back to z. NOTE they are half the length, but can just pull the top half of the list for plotting. 

dif <- z[ z$Model == '100 m', ]$TSS - z[ z$Model == '20 m', ]$TSS
z <- cbind( z, 'diffTSS' = dif )

dif <- z[ z$Model == '100 m', ]$TPR - z[ z$Model == '20 m', ]$TPR
z <- cbind( z, 'diffTPR' = dif )

dif <- z[ z$Model == '100 m', ]$TNR - z[ z$Model == '20 m', ]$TNR
z <- cbind( z, 'diffTNR' = dif )

#Plot.TSS.By.Depth.For.Regions(  z[ , c('Model', 'Region', 'Ribbon', 'TSS')], pal.cb2 )
y <- z[ 1:40, c('Model', 'Region', 'Ribbon', 'diffTSS')]
names(y)[4] <- 'diff'
Plot.Stat.By.Depth.For.Regions( y, 'TSS', pal.cb2[2] )

```

**Figure 6a: **  

```{r Fig6b_Depth_Results_by_Resolution_TPR, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}
y <- z[ 1:40, c('Model', 'Region', 'Ribbon', 'diffTPR')]
names(y)[4] <- 'diff'
Plot.Stat.By.Depth.For.Regions( y, 'Accuracy', pal.cb2[2] )

```

**Figure 6b: **  

```{r Fig6c_Depth_Results_by_Resolution_TNR, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}
y <- z[ 1:40, c('Model', 'Region', 'Ribbon', 'diffTNR')]
names(y)[4] <- 'diff'
Plot.Stat.By.Depth.For.Regions( y, 'TNR', pal.cb2[2] )

```

**Figure 6: Difference in the (A) true skill statistic (TSS), (B) overall Accuracy, and (C) true negative rate (TNR) between the 20 and 100 m models across depth ranges, by region, using the build testing partition. Values below 0 indicate a higher score by the 20 m model. As these are difference plots, no corrections for the random baseline are required.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig7a_Depth_Results_Obs_Pontius, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results[ depth.results$Model == '100 m', ]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.Depth.For.Regions( z, rev(pal.cb4) )
```

**Figure 7a:**  

```{r Fig7b_Depth_Results_Obs_Pontius, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results[ depth.results$Model == '20 m', ]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.Depth.For.Regions( z, rev(pal.cb4) )
```

**Figure 7b: Regional performance of (A) the 100m model and (B) the 20 m regional models across depth zones, using the build testing partition.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


**Figure 8: Mapped predictions for A) two 20 m regional models (HG and NCC) and B) the 100 m coastwide model. Note the detail provided by the 20 m models near shore where the 100 m model predicts largely Rock. In contrast, the 100 m model captures known features at depth not captured by the 20 m model.** 


```{r Fig9_IDS_One, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}

# 3) Look at Integrated statistics for each IDS by Region.
# Use a predetermined collection of stats (see function)

# Again, start with a little data prep ... 

y <- IDE.results.wtd$Integrated
z <- y[ y$Region %in% c('Coast', 'HG', 'NCC'), ]

# This function can be used to generate various views of the data. 
# 2020/05/25 DH preferred IDS as the grouping variable ... 
# 2020/08/23 Standardized them all as the difference from random baseline
Plot.Stats.By.IDS.For.Regions( y, pal.cb3b, sz = 25, lx=0.85, ly=0.87 )

# To save  hi res image for manuscript ... save above to 'a', and ... 
#ggsave( paste0( 'Figure 9 - Integrated Stats by IDE for Regions.png'), a, dpi = 300, width = 16, height = 10, path = results.dir)
```

**Figure 9: Integrated, accuracy metrics of model predictive power for each independent data set by Region. The metrics are shown as the difference from the random baseline. Thus positive values indicate performance better than random and negative values indicate performance worse than random. In a multi-class error matrix, the random baseline differs for each statistic: for the true skill statistic (TSS) it is 0.5, for the true positive rate (TPR) it is 0.25, and for the true negative rate (TNR) it is 0.75. Where bars are not shown, the difference was 0, indicating random performance. **  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig10a_IDS_By_Ribbon_A, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
# Pontius stats (Accuracy, Quantity, Exchange, Shift) for IDS, by ribbon, 
# one plot for each region.
# Remember: Accuracy = 1 – (Quantity + Exchange + Shift).

z <- IDE.depths[ IDE.depths$Region == 'Coast', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  

**Figure 10a: **  

```{r Fig10b_IDS_By_Ribbon_B, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
z <- IDE.depths[ IDE.depths$Region == 'HG', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  

**Figure 10b: **  

```{r Fig10c_IDS_By_Ribbon_C, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
z <- IDE.depths[ IDE.depths$Region == 'NCC', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  

**Figure 10c: Error assessment of model predictions of the independent data sets by depth zone for the A) Coastwide, B) Haida Gwaii, and C) North Central Coast models (the three regions with sufficient sample size).**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r FigXXa_IDEbyClass, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
# A simple region-based facet of class-based accuracy and TNR. 
# Build the data, then 3 plots.

a <- IDE.results.wtd$PerClass
b <- a[ a$Stat   %in% c('TPR', 'TNR'), ]
c <- b[ b$Region %in% c('Coast', 'HG', 'NCC'), ]
row.names(c) <- NULL
colnames(c)[ colnames(c) %in% c('1', '2', '3', '4') ] <- c('Hard', 'Mixed', 'Sand', 'Mud')

# Take apart and put back together after adjusting for different random baselines

d <- rbind( 
  cbind( 
    c[ c$Stat == 'TPR', c('Region', 'IDS')], 'Stat' = 'Accuracy',  
    c[ c$Stat == 'TPR', c('Hard', 'Mixed', 'Sand', 'Mud') ] - 0.25 ),
  cbind( 
    c[ c$Stat == 'TNR', c('Region', 'IDS', 'Stat')],  
    c[ c$Stat == 'TNR', c('Hard', 'Mixed', 'Sand', 'Mud') ] - 0.75 )
)

Plot.ClassStats.IDE( d[ d$IDS == 'Dive', -grep('IDS', colnames(c)) ], 'Dive', pal.cb2, sz = 25 )

```  

**Figure XXa: **  

```{r FigXXb_IDEbyClass, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
Plot.ClassStats.IDE( d[ d$IDS == 'Cam', -grep('IDS', colnames(c)) ], 'Cam', pal.cb2, sz = 25 )

```

**Figure XXb: **  

```{r FigXXc_IDEbyClass, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}

Plot.ClassStats.IDE( d[ d$IDS == 'ROV', -grep('IDS', colnames(c)) ], 'ROV', pal.cb2, sz = 25 )

```

**Figure XXc: Accuracy assessment of the predictive power of the Coastwide, Haida Gwaii (HG), and North Central Coast (NCC) models against the A) Dive, B) Camera, and C) ROV independent data sets.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


# Supplemental tables (6) and figures (2)

```{r Supp_Tables, echo=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3) 

T.cap <- "Table S3: Error matrices for the six weighted models. Prevalence (Prev), User and Producer (Prod) accuracies are provided to interpret model fit (Predictions) to the build testing data (Reference)."  

x <- Error.Matrix( rf.region.Coast, obs.100mIV[ obs.100mIV$TestDat == 1, ] )
kable( x , digits=2, caption = T.cap )
cat('Coastwide')

x <- Error.Matrix( rf.region.HG,   obs.20mIV$HG[  obs.20mIV$HG$TestDat == 1, ] )
kable( x , digits=2 )
cat('HG')

x <- Error.Matrix( rf.region.NCC,   obs.20mIV$NCC[  obs.20mIV$NCC$TestDat == 1, ] )
kable( x , digits=2 )
cat('NCC')

x <- Error.Matrix( rf.region.WCVI,   obs.20mIV$WCVI[  obs.20mIV$WCVI$TestDat == 1, ] )
kable( x , digits=2 )
cat('WCVI')

x <- Error.Matrix( rf.region.QCS,   obs.20mIV$QCS[  obs.20mIV$QCS$TestDat == 1, ] )
kable( x , digits=2 )
cat('QCS')

x <- Error.Matrix( rf.region.SOG,   obs.20mIV$SOG[  obs.20mIV$SOG$TestDat == 1, ] )
kable( x , digits=2 )
cat('SOG')

```

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r FigS1_Prevalence_IDS, fig.width=10, fig.height=6, out.width='120%', echo=FALSE}
# Depends on IDE.results created in main script. 
#--- 1) Begin by looking at IDS sample sizes using perClass prevalences 
x <- IDE.results.wtd$PerClass
y <- x[ x$Stat == 'PrevObs', ]

# Tidy the data a bit  ... 
y <- x[ x$Stat == 'PrevObs', ] %>% subset(select = -c(Stat))
colnames(y) <- c( 'Region', 'IDS', 'Rock', 'Mixed', 'Sand', 'Mud')
rownames(y) <- NULL

# fill in  blank data to balance plot ... 
y <- rbind( y, data.frame( 'Region' = 'WCVI', 'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0),
               data.frame( 'Region' = 'QCS',  'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0),
               data.frame( 'Region' = 'SOG',  'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0) )

Plot.Obs.By.IDS.For.Regions( y, pal.cb3b, sz = 25, lx=0.83, ly=0.75 )

```

**Figure S1: Prevalence of independent data sets by depth class, across regions. **  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r FigS2_Prevalence_ObsPred, fig.width=10, fig.height=6, echo=FALSE, out.width='120%' }
#-- Plots the Study Area prevalence results. 
#   Requires the map.prev and build.sum data structure for some data prep. 

Plot.Pred.Map.Prevalence( map.prev, build.sum, pal.RMSM, sz = 25)
```

**Figure S2: Comparison of prevalence in predictions between fitted model (Points) and study area (Map).**    

Mapped prevalences are a reasonable reflection of true prevalences by region  

Appeal to author experience. And also technical argument why this would be so. e.g., stationarity across process; we know there is sample bias, would be disturbing to see that reflected in the predicted surface. Shows model is working! And that RF method is dealing well with prevalence.   

Include illustration of value of wtd RF. Good conclusion, emphasizing the importance of dealing with prevalence in models. Table/Fig for suppl. Materials.  

Standardize terminology with earlier figure that compares build/test prevalences   

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


### S2. Analytical methods  

## Overview   

This documentation describes the R code used to conduct the analyses reported in Gregr et al. (Comprehensive, coastwide marine substrate classification). 

The analysis loads point data and polygon data from ArcGIS files. These source data are not provided, but their imported representations are packaged with this code. 

These data structures include the build data and the independent data sets described in the main text, with the 20 m and 100 m predictor variables attached. This will allow the analysis to be reproduced, however the study area-wide predictions will not be reproducable. 

# Data processing steps (IDE_Main.R)   

*Data loading*   
Source data are imported from ArcGIS geodatabase features. These include the build data (points), with the 100 m predictors attached, rasters of regional 20 m predictors, and the independent data (Dive, Cam, ROV) for evaluation as points.   
	
Relevant data structures: *obs.100mIV, obs.20mIV, Dive.20mIV, Cam.20mIV, ROV.20mIV*.    
These data are not included in the distributed code package.  

*Build and evaluate RF models*  
Create the weighted and unweighted RF models; summarize the results for model fit to the build data, and for the tests against independent data; and summarize the results for model performance tests across regions and depths.   

Weighted random forest structures: *rf.region.Coast, rf.region.HG, rf.region.NCC, rf.region.WCVI,	rf.region.QCS, rf.region.SOG*    

Non-weighted random forest structures: *nwrf.region.Coast, nwrf.region.HG, nwrf.region.NCC, nwrf.region.WCVI,	nwrf.region.QCS, nwrf.region.SOG*   

*build.results* is a multi-layered list including agregated and class-based results. *build.sum* contains tablulated summaries of the build results; *IDE.results.wtd* and *IDE.results.nowt* are multi-layerd lists including agregated and class-based results for both the weighted, and non-weighted models.  

*Independent data evaluation*  
Evaluation of all 6 models against the 3 independent data sets.  

*Tests of stationarity*  
Evaluate effect of model resolution across depths, in each region. For the regional models, test their performance by depth. These tests used the testing partition of the build data for consistency.   

# Management and presentation of results   

All data structures are saved as .RData files, presentation is managed by the rMarkdown script. Metrics that re dependent on a random baseline (TSS, Accuracy, TNR) are reported as native values in tables, but are scaled for figures as necessary to facilitate interpretation.  

The R code includes the following files:  
*IDE_Main.R*: The control script that sources necessary functions and libraries, and controls the loading of the data.  

*build_substrate.R*: The high level build work (function calling and result building) behind IDE_Main.R.   

*substrate_functions.R*: Functions for data loading, data analysis, and result building.   

*plot_functions.R*: Plotting functions ... currently more than u need.    

*substrate_figures.RMD*: The Markdown script generates plots from existing data objects in the environment, or pulls graphics created (ArcGIS) or assembled (ppt) elsewhere. The code chunks also do some modifying (e.g., scaling of the TSS) of the summarized data.   

To run the Markdown script, Knitr needs to see the necessary data objects. Easiest to run the script from the console (>) as:   
x <- substr(  Sys.time(), 1, 10)   
rmarkdown::render( "substrate_figures.Rmd",   
  output_file = paste0('SubstrateFigures_', x ),  
  output_dir = results.dir )   

## R libraries used   
caret (Kuhn 2020): Contingency table and metrics   
Colorbrewer (Neuwirth 2014): Colour selction including colour-blind palettes   
superheat (Barter and Yu 2017): Heat maps   
ggplot2 (Wickham 2016): Bar plots   
reshape2 (Wickham 2007): Shaping data for ggplot2    
tidyr (Wickham and Henry 2020): Data manipulation functions   
dplyr (Wickham et al. 2020): Data manipulation functions   
diffeR (Pontius Jr. and Santacruz 2019): Error metrics    
ranger (Wright and Ziegler 2017): Random forest implementation   
rgdal (Bivand et al.2020): Geospatial package   
raster (Hijmans (2020): Geographic Data Analysis and Modeling   
stringr (Wickham 2007): String operations  
e1071 (Meyer et al. 2019): Misc functions   
forcats (Wickham 2020): Tools for categorical variables   
measures (Probst 2018): Performance measures   
vegan (Oksanen 2019): Community ecology   
PresenceAbsence (Freeman Moisen 2008): Presence-absence model analysis   


## References
Freeman, E. A. and Moisen, G. (2008). PresenceAbsence: An R Package for Presence-Absence Model Analysis. Journal of Statistical Software, 23(11):1-31. http://www.jstatsoft.org/v23/i11

Jari Oksanen, F. Guillaume Blanchet, Michael Friendly, Roeland Kindt, Pierre Legendre, Dan McGlinn, Peter R. Minchin, R. B. O'Hara, Gavin L. Simpson, Peter Solymos, M. Henry H. Stevens, Eduard Szoecs and Helene Wagner (2019). vegan: Community Ecology Package. R package version 2.5-6. https://CRAN.R-project.org/package=vegan

Philipp Probst (2018). measures: Performance Measures for Statistical Learning. R package version 0.2. https://CRAN.R-project.org/package=measures

Wickham, Hadley (2020). forcats: Tools for Working with Categorical Variables (Factors). R package version 0.5.0. https://CRAN.R-project.org/package=forcats
  
David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel and Friedrich Leisch (2019). e1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. R package version 1.7-3. https://CRAN.R-project.org/package=e1071
  
Wickham, Hadley (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. https://CRAN.R-project.org/package=stringr
  
Robert J. Hijmans (2020). raster: Geographic Data Analysis and Modeling. R package version 3.3-13. https://CRAN.R-project.org/package=raster
  
Roger Bivand, Tim Keitt and Barry Rowlingson (2020). rgdal: Bindings for the 'Geospatial' Data Abstraction Library. R package version 1.5-12. https://CRAN.R-project.org/package=rgdal
 
Marvin N. Wright, Andreas Ziegler (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1-17. doi:10.18637/jss.v077.i01

Wickham, Hadley (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.

Wickham, Hadley and Lionel Henry (2020). tidyr: Tidy Messy Data. R package version 1.1.0. https://CRAN.R-project.org/package=tidyr

Wickham, Hadley, Romain François, Lionel Henry and Kirill Müller (2020). dplyr: A Grammar of Data Manipulation. R package version 1.0.0. https://CRAN.R-project.org/package=dplyr

Wickham, Hadley (2007). Reshaping Data with the reshape Package. Journal of Statistical Software, 21(12), 1-20. URL http://www.jstatsoft.org/v21/i12/.
  
Erich Neuwirth (2014). RColorBrewer: ColorBrewer Palettes. R package version 1.1-2. https://CRAN.R-project.org/package=RColorBrewer

Rebecca Barter and Bin Yu (2017). superheat: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. R package version 0.1.0. https://CRAN.R-project.org/package=superheat

Robert Gilmore Pontius Jr. and Ali Santacruz (2019). diffeR: Metrics of Difference for Comparing Pairs of Maps or Pairs of Variables. R package version 0.0-6. https://CRAN.R-project.org/package=diffeR
  
Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-86. https://CRAN.R-project.org/package=caret


  
## Markdown resources
Details on using R Markdown  <http://rmarkdown.rstudio.com>  
On defining a style to serve as a pagebreak  <https://datascienceplus.com/r-markdown-how-to-insert-page-breaks-in-a-ms-word-document/>  
Guidance for landscape tables/pages <https://code-examples.net/en/q/18a6fd6>  
Messing with figure sizing <https://sebastiansauer.github.io/figure_sizing_knitr/>  
Some info on KNITR and environment <https://stackoverflow.com/questions/34029611/how-to-use-objects-from-global-environment-in-rstudio-markdown>
The definitive guide <https://bookdown.org/yihui/rmarkdown/>