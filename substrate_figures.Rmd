---
title: "Substrate Tables and Figures"
author: "Edward Gregr"
date: "2020/09/01"
output: word_document
---

```{r Build_Tables, echo=FALSE}
library( knitr )
knitr::opts_chunk$set(fig.width=12, fig.height=8) 

#-- Main statistics don't make a very nice bar plot. Report as table. 
T.cap <- "Table 3: Aggregated model build metrics for all models, showing the weighted random forest results in the first row, and the unweighted results in the second row. Out of bag (OOB) values show the mean prediction error from the random forest re-sampling. The true skill statistic (TSS), true positive rate (TPR), and true negative rate (TNR) express by how model performance exceeds random, while accuracy, quantity, exchange and shift provide an assessment of model error. See supplementary materials for more details." 

x <- rbind( build.sum$build.results.Integrated,
            build.sum.nw$build.results.Integrated )

# adjust region levels ... arguably should be much earlier ... 
levels( x$Region ) <- fct_relevel(x$Region, "Coast", "HG", "NCC", "WCVI", "QCS", "SOG" )

x <- x[ order(x$Region),]
x <- x[ , c( 'Region', 'N', 'Imbalance', 'OOB', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift')]
names(x)[7] <- 'Specificity'
row.names(x) <- NULL

kable( x , digits=2, caption = T.cap )


# Create table of how 100 m coastal model predicts at regional scale. 
# REQUIRES test.regions to exist. 
T.cap <- "Table 4: Aggregated metrics for the 100 m model built for each region (see Table 1 for a description of the metrics). Regional sample sizes differ slightly from Table 1 because for this comparison the build data were separated using a regional shape file, whereas in Table 1 the points were selected to region using regional 20 m rasters." 

x <- rbind( 
    cbind( 'Region' = 'Coast', Results.Row( rf.region.Coast, test.regions$Coast )$Integrated), 
    cbind( 'Region' = 'HG',    Results.Row( rf.region.Coast,    test.regions$HG )$Integrated), 
    cbind( 'Region' = 'NCC',   Results.Row( rf.region.Coast,   test.regions$NCC )$Integrated),
    cbind( 'Region' = 'WCVI',  Results.Row( rf.region.Coast,  test.regions$WCVI )$Integrated),
    cbind( 'Region' = 'QCS',   Results.Row( rf.region.Coast,   test.regions$QCS )$Integrated),
    cbind( 'Region' = 'SOG',   Results.Row( rf.region.Coast,   test.regions$SOG )$Integrated)
  )

x <- x[, !(names(x) %in% c("OOB")) ]
x <- x[ , c( 'Region', 'N', 'Imbalance', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift')]
names(x)[6] <- 'Specificity'
row.names( x ) <- NULL

kable( x, digits=2, caption = T.cap )


# Table of IDE results. 

T.cap <- "Table 5: Performance of each random forest model for each independent data set." 

#- Build an IDE results table for the Wtd model ... 

x <- IDE.results.wtd$Integrated
y <- rbind( x[x$IDS=='Dive',], 
            x[x$IDS=='Cam',],
            x[x$IDS=='ROV',]
  )
z <- cbind( 'IDS'=y$IDS, y[,-2] )
rownames( z ) <- NULL

x <- z
x <- x[, !(names(x) %in% c("OOB")) ]
x <- x[ , c( 'IDS', 'Region', 'N', 'Imbalance', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift')]
names(x)[7] <- 'Specificity'
row.names( x ) <- NULL

kable( x, digits=2, caption = T.cap )


# Build ANOVA from depth.results argument ... 

# # Copied from IDE_Main.R ... 
# names(depth.results)
# cor( depth.results[, -c(1:3)] )
# 
# # replace TSS w other metrics to create results. 
# a <- lm( TSS ~ Model + Region + Ribbon + N + Imbalance, data = depth.results )
# summary(a)
# anova( a )

```   


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Fig_1_Boundaries, echo=FALSE}

#-- Insert pre-built boundary file as png.
```

![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/figures/boundaries.png){ fig.fullwidth=TRUE }

**Figure 1: Map of study area showing outline of the shelf (for the 100 m coastwide model) and the five regions including Haida Gwaii (HG), North Central Coast (NCC), West Coast of Vancouver Island (WCVI), Queen Charlotte Strait (QCS), and the Strait of Georgia (SOG).**

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Fig_2_Build_Heatmaps, echo=FALSE}

#--- Heatmaps (tigures!) of build stats. 
# As superheat library only makes pngs, they have to be pre-built and then just called here. 

# 2020/09/04: Now pre-built multi-panel from the heatmap output.

```
![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/figures/ms_figure2_classheat.png)

**Figure 2: Heatmaps of class-based true positive rate (top) and true negative rate (bottom) for weighted (left column) and non-weighted (right column) models. The colour shading within each row reflects the underlying numbers from high (red) to low (blue) and is included to make the differences in values more readily apparent.**  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig_3_Build_Prevalence, echo=FALSE }

#-- Prevalence plots (obs & predicted) for build (testing partition), faceted by Region
a <- build.sum$build.results.ClassPrev
b <- build.sum.nw$build.results.ClassPrev[7:12,]

# change numbers to proportions
x <- a[, names(a) %in% c('Hard', 'Mixed', 'Sand', 'Mud')]
y <- data.frame(  x / rowSums( x ))

# add results back to the Region column
z <- cbind( 'Region' = a$Region, 'Source' = c(rep('Observed',6), rep('Weighted',6)), y)

#--- do it again with the non-wtd model, this time keep only the predictions ... 

x <- b[, names(b) %in% c('Hard', 'Mixed', 'Sand', 'Mud')]
y <- data.frame(  x / rowSums( x ))
zz <- cbind( 'Region' = b$Region, 'Source' = rep('No Weights',6), y)

foo <- rbind( z, zz)

Plot.Obs.Pred.Prevalence.Build( foo, pal.cb3b, sz=30, lx=0.15, ly=0.85 ) 

# save plot to 'a' if using ggsave()
# ggsave("Class Prevalence for Obs&Pred (Build) for Regions.png", a, dpi = 300, width = 16, height = 10, path = results.dir)

```

**Figure 3: Observed class prevalences for the build testing partition compared to predictions from the weighted and unweighted random forest models across regions.**  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/heat_VariableImportance_Build.png){ fig.fullwidth=TRUE }

**Figure 4: Heatmap of variable importance across models. Importance is defined as the proportion of each predictor's contribution to the model, relative to the predictor with the highest contribution. (p/max(p)). The colour shading within each row reflects the underlying numbers from high (red) to low (blue) and is included to make the differences in the values more readily apparent. NA indicates that Fetch was not used as a predictor for the Coast model.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Fig5_Model_Compare, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
#-- Does using just the top 6 predictors improve RF performance against the IDS?
#   Rolled up with BoPs data as figure

#-- COMBINE the alternate models into a table of IDE results ... 
a <- IDE.results.wtd$Integrated
a <- a[, c('IDS', 'Region', 'N', 'Imbalance', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift') ]
a <- a[ a$IDS %in% c('Dive', 'Cam'), ]

a <- data.frame( a[ a$Region %in% c('HG', 'NCC', 'WCVI', 'QCS', 'SOG'), ])
a <- cbind( a[, c(1,2)], 'Model' = 'Full', a[, c(3:10)] )

b <- IDE.results.trm$Integrated
b <- b[, c('IDS', 'Region', 'N', 'Imbalance', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift') ]
b <- b[ b$IDS %in% c('Dive', 'Cam'), ]

b <- data.frame( b[ b$Region %in% c('HG', 'NCC', 'WCVI', 'QCS', 'SOG'), ])
b <- cbind( b[, c(1,2)], 'Model' = 'Trimmed', b[, c(3:10)] )

c <- IDE.BoP[, c('IDS', 'Region', 'N', 'Imbalance', 'TSS', 'Accuracy', 'TNR', 'Quantity', 'Exchange', 'Shift') ]
c <- cbind( c[, c(1,2)], 'Model' = 'BoP', c[, c(3:10)] )

d <- rbind( a, b, c )

model.compare <- d[ with(d, order(IDS, Region)), ]
row.names(model.compare) <- NULL


Plot.TSS.By.IDS.For.Regions( model.compare, pal.cb3b, sz = 25, lx=0.85, ly=0.87 )

```

**Figure 5: True skill statistic (scaled to a random baseline of 0.5) assessing the forecast skill of three models (the full random forest (RF) model, a RF model trimmed to use only the 6 most influential predictors, and the bottom patch (BoP) object-based model) against the Dive and Camera (Cam) independent data sets, for all 20 m regional models.  **  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig6_IDS_One, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}

# 3) Look at Integrated statistics for each IDS by Region.
# Use a predetermined collection of stats (see function)

# Again, start with a little data prep ... 

y <- IDE.results.wtd$Integrated
z <- y[ y$Region %in% c('Coast', 'HG', 'NCC'), ]

# This function can be used to generate various views of the data. 
# 2020/05/25 DH preferred IDS as the grouping variable ... 
# 2020/08/23 Standardized them all as the difference from random baseline
Plot.Stats.By.IDS.For.Regions( z, pal.cb3b, sz = 25, lx=0.85, ly=0.87 )

# To save  hi res image for manuscript ... save above to 'a', and ... 
#ggsave( paste0( 'Figure 8 - Integrated Stats by IDE for Regions.png'), a, dpi = 300, width = 16, height = 10, path = results.dir)
```

**Figure 5: Integrated, better-than-random metrics for independent data sets by Region. The metrics are shown as the difference to the random baseline. Thus positive values indicate performance better than random and negative values indicate performance worse than random. In a multi-class error matrix, the random baseline differs for each statistic: for the true skill statistic (TSS) it is 0.5, for the true positive rate (TPR) it is 0.25, and for the true negative rate (TNR) it is 0.75. Where bars are not shown, the difference was 0, indicating random performance.  **  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig7_IDS_Two, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}

# What about TSS vs. Quantity and Allocation?
# Remember: Accuracy = 1 – (Quantity + Exchange + Shift).

y <- IDE.results.wtd$Integrated
z <- y[ y$Region %in% c('Coast', 'HG', 'NCC'), ]

z$Allocation <- z$Shift + z$Exchange
z <- z[, c( 'Region', 'IDS', 'Accuracy', 'Quantity', 'Exchange','Shift' )]

Plot.Pontius.By.IDS.For.Regions( z, rev(pal.cb4), sz = 25 )
```

**Figure 7: Error assessment statistics for independent data sets by region.**  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig8a_Depth_Results_by_Resolution_TSS, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results

# Build some difference columns, and add them back to z. NOTE they are half the length, but can just pull the top half of the list for plotting. 

dif <- z[ z$Model == '100 m', ]$TSS - z[ z$Model == '20 m', ]$TSS
z <- cbind( z, 'diffTSS' = dif )

dif <- z[ z$Model == '100 m', ]$TPR - z[ z$Model == '20 m', ]$TPR
z <- cbind( z, 'diffTPR' = dif )

dif <- z[ z$Model == '100 m', ]$TNR - z[ z$Model == '20 m', ]$TNR
z <- cbind( z, 'diffTNR' = dif )

#Plot.TSS.By.Depth.For.Regions(  z[ , c('Model', 'Region', 'Ribbon', 'TSS')], pal.cb2 )
y <- z[ 1:40, c('Model', 'Region', 'Ribbon', 'diffTSS')]
names(y)[4] <- 'diff'
Plot.Stat.By.Depth.For.Regions( y, 'TSS', pal.cb2[2] )

```

**Figure 8a: Difference in the true skill statistic (TSS) between the 20 and 100 m models across depth ranges, by region (using the build testing partition).**  

```{r Fig8b_Depth_Results_by_Resolution_TPR, echo=FALSE}
y <- z[ 1:40, c('Model', 'Region', 'Ribbon', 'diffTPR')]
names(y)[4] <- 'diff'
Plot.Stat.By.Depth.For.Regions( y, 'TPR', pal.cb2[2] )

```

**Figure 8b: Difference in the true positive rate (TPR) between the 20 and 100 m models across depth ranges, by region (using the build testing partition).**  

```{r Fig8c_Depth_Results_by_Resolution_TNR, echo=FALSE}
y <- z[ 1:40, c('Model', 'Region', 'Ribbon', 'diffTNR')]
names(y)[4] <- 'diff'
Plot.Stat.By.Depth.For.Regions( y, 'TNR', pal.cb2[2] )

```

**Figure 8c: Difference in the true negative rate (TNR) between the 20 and 100 m models across depth ranges, by region (using the build testing partition).**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Fig9a_Depth_Results_Obs_Pontius, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results[ depth.results$Model == '100 m', ]

# Change levels so Coast is first ... 
z <- mutate( z, Region = forcats::fct_relevel( z$Region, c('HG', 'NCC', 'WCVI', 'QCS', 'SOG') ))

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.Depth.For.Regions( z, rev(pal.cb4) )
```

**Figure 9a: Performance of 100 m coastwide model by region, across depth zones, using the build testing partition.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Fig9b_Depth_Results_Obs_Pontius, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results[ depth.results$Model == '20 m', ]

# Change levels so Coast is first ... 
z <- mutate( z, Region = forcats::fct_relevel( z$Region, c('HG', 'NCC', 'WCVI', 'QCS', 'SOG') ))

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.Depth.For.Regions( z, rev(pal.cb4) )
```

**Figure 9b: Performance of 20 m regional models across depth zones, using the build testing partition.**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Fig10a_IDS_By_Ribbon_A, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
# Pontius stats (Accuracy, Quantity, Exchange, Shift) for IDS, by ribbon, 
# one plot for each region.
# Remember: Accuracy = 1 – (Quantity + Exchange + Shift).

z <- IDE.depths[ IDE.depths$Region == 'Coast', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  

**Figure 10a: Accuracy and error assessment statistics for independent data sets by depth zone - Coastwide.**  


```{r Fig10b_IDS_By_Ribbon_B, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
z <- IDE.depths[ IDE.depths$Region == 'HG', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  

**Figure 10b: Accuracy and error assessment statistics for independent data sets by depth zone - Haida Gwaii.**  

```{r Fig10c_IDS_By_Ribbon_C, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
z <- IDE.depths[ IDE.depths$Region == 'NCC', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]

#Adjust accuracy so bars are all the same height 
z$Accuracy <- 1 - (z$Shift + z$Exchange + z$Quantity)

Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  

**Figure 10c: Accuracy and error assessment statistics for independent data sets by depth zone - North Central Coast.**  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Fig11_Prevalences, echo=FALSE, fig.fullwidth=TRUE }
#-- Plots the Study Area prevalence results. Requires the map.prev structure. 
Plot.Pred.Map.Prevalence( map.prev, build.sum, pal.RMSM)
```

**Figure 11: Comparison of prevalence in predictions between fitted model (points) and study area (Map).**    

Mapped prevalences are a reasonable reflection of true prevalences by region  

Appeal to author experience. And also technical argument why this would be so. e.g., stationarity across process; we know there is sample bias, would be disturbing to see that reflected in the predicted surface. Shows model is working! And that RF method is dealing well with prevalence.   

Include illustration of value of wtd RF. Good conclusion, emphasizing the importance of dealing with prevalence in models. Table/Fig for suppl. Materials.  

Standardize terminology with earlier figure that compares build/test prevalences   

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


## Supplemental tables and figures


```{r Weight_comparison_integrated_table, echo=FALSE}
library( knitr )
# Build a weighted non-weighted comparision table. 
a <- build.sum$build.results.ByClass[ build.sum$build.results.ByClass$Stat %in% c('TPR', 'TNR', 'User'), ]
a <- cbind( 'Model' = 'Weighted', a)
a <- a[ order( a$Region ), ]

b <- build.sum.nw$build.results.ByClass[ build.sum$build.results.ByClass$Stat %in% c('TPR', 'TNR', 'User'), ]
b <- cbind( 'Model' = 'No weights', b)

c <- rbind( a,b )
c <- c[ order( c$Region ), c('Region', 'Model', 'Stat', 'Hard', 'Mixed', 'Sand', 'Mud') ]
row.names(c) <- NULL

T.cap <- "Table S1: Class-based true positive (TPR/Accuracy) and true negative (TNR/Specificity) rates, and reliability (User Accuracy) for all regions, comparing weighted and non-weighted random forest models." 
kable( c, digits=2, caption = T.cap )
```


```{r IDE_byClass_allRegions_table, echo=FALSE}
library( knitr )

T.cap <- "Table S2: Class-based assessment of independent data evaluation showing true positive (TPR/Accuracy) and true negative (TNR/Specificity) rates, and reliability (User Accuracy) for all regions, for all independent data sets."

x <- rbind(
  cbind( 'Model' = 'Wtd',   IDE.results.wtd$PerClass[ IDE.results.wtd$PerClass$IDS == 'Dive', ]),
  cbind( 'Model' = 'NoWts', IDE.results.nowt$PerClass[ IDE.results.nowt$PerClass$IDS == 'Dive', ]),
  cbind( 'Model' = 'Trim',  IDE.results.trm$PerClass[ IDE.results.trm$PerClass$IDS == 'Dive', ]),
  cbind( 'Model' = 'Wtd',   IDE.results.wtd$PerClass[ IDE.results.wtd$PerClass$IDS == 'Cam', ]),
  cbind( 'Model' = 'NoWts', IDE.results.nowt$PerClass[ IDE.results.nowt$PerClass$IDS == 'Cam', ]),
  cbind( 'Model' = 'Trim',  IDE.results.trm$PerClass[ IDE.results.trm$PerClass$IDS == 'Cam', ])
)

x <- x[ x$Stat %in% c( 'TPR', 'TNR', 'User'), ]
colnames(x) <- c('Model', 'Region', 'IDS', 'Stat', 'Hard', 'Mixed', 'Sand', 'Mud')

y <- x[ , c('IDS', 'Region', 'Stat', 'Model', 'Hard', 'Mixed', 'Sand', 'Mud') ]

#y <- mutate(y, Scale = factor(Stat, levels=c("Points", "Map")))
y <- mutate( y, Stat = forcats::fct_relevel( y$Stat, c('TPR', 'TNR', 'User') ))

z <- y[ with(y, order(IDS, Region, Stat, Model)), ]
row.names(z) <- NULL

kable( z, digits=2, caption = T.cap )

```   


```{r Model_comparison_integrated_table, echo=FALSE}
library( knitr )

T.cap <- "Table S3: Integrated performance metrics for three models (the full random forest (RF) model, a RF model trimmed to use only the 6 most influential predictors, and the bottom patch (BoP) object-based model) against the Dive and Camera (Cam) independent data sets, for all 20 m regional models." 

# USES model.compare built above for Fig 5
kable( model.compare, digits=2, caption = T.cap )

```   


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r IDS_One, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
# Depends on IDE.results created in main script. 
#--- 1) Begin by looking at IDS sample sizes using perClass prevalences 
x <- IDE.results.wtd$PerClass
y <- x[ x$Stat == 'PrevObs', ]

# Tidy the data a bit  ... 
y <- x[ x$Stat == 'PrevObs', ] %>% subset(select = -c(Stat))
colnames(y) <- c( 'Region', 'IDS', 'Rock', 'Mixed', 'Sand', 'Mud')
rownames(y) <- NULL

# fill in  blank data to balance plot ... 
y <- rbind( y, data.frame( 'Region' = 'WCVI', 'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0),
               data.frame( 'Region' = 'QCS',  'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0),
               data.frame( 'Region' = 'SOG',  'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0) )

Plot.Obs.By.IDS.For.Regions( y, pal.cb3b, sz = 25, lx=0.83, ly=0.75 )

```

**Figure S1: Prevalence of independent data sets by depth class, across regions. **  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Overview 

Since the meeting on 2020/06/18:
- colour-blind palette done
    Now hard to find text that works on all heat map colours
    Obs/Pred and Model/Map pairs could have different (but consistent colours)
    
- TSS difference done for depth classes
- TSS scaled on all plots and tables

TO DO:  
- Create new Suppl. materials section.   
	SUPPLEMENTAL FIGURES   
		S1 - S5: Regional figures showing build data density.
		
- Pontius has different names for User/Producer accuracies (commission/omission rates)  
Need to standardize on the best stats terminology (Joanne/Ed to work on this)  
- Confirm TSS scaling working properly. More code walk thrus.  
- Update following page on data processing. Update links btwn dataframes and rmd script.   
- Need decision re density comparison. Necessary? Its conflated with depth. Does this matter?  
Worth doing by Region? How best to structure? (e.g., pick an earlier figure as a template?).

Group interpretation (2020/06/18):   
- Higher dens sampling areas closer to shore, and more heterogeneity close to shore. Hi dens required to capture this hetro, and so these areas better captured  
- High density areas more heterogeneous, this thought to potentially confuse the model, leading to reduced accuracy.   
- Revisit with top/bottom 25%  
- Recall that point subsetting by region can now be linked back to pt features using ID 

Two parts: Density of points has higher imbalance ? but we are not dealing with complexity of the coastline, which we would see, perhaps if we did it by region.   


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```





## Data processing steps

All data structures are saved as .RData files. 

### IDE_Main.R

**PART 1: Load, prep, and inspect all the data.**   
	Includes build data w 100 m predictors; All IDS data; Regional 20 m predictors
	
Data structures: *point.data, dive.20mIV, cam.20mIV, ROV.20mIV, obs.100mIV, obs.20mIV*  
Standardized predictor names: *names.100m*  

**PART 2: Build and evaluate the required RF models.**  
	Creates RF model structures, and summarizes Build statistics

Data structures: rf.region.Coast, rf.region.HG, rf.region.NCC, rf.region.WCVI,  
	rf.region.QCS, rf.region.SOG  
Build summaries: *build.results*   

CSV Files: Build_results_Integrated.csv - build.results.Integrated
	   Build_results_byClassStats.csv - build.results.ByClass
	   Build_results_testPrevalence.csv - build.results.ClassPrev
	   Build_results_varImportance.csv - build.results.VarImport


**Part 3: Produce and save heatmaps (4) of build results**  


**Part 4: Model resolution tests**  


**Part 4b: Model resolution across depths**  
Take the Coastwide model, test its performance by depth, in each region.  
Take the regional models, test their performance by depth, in each region.  
(Coastwide is 100m, regional are 20 m. Points are the same)

CSV Files: Stats_byRibbon_byRegion_byModel.csv
	   TSS_byRibbon_byRegion_byModel.csv (for heatmaps)


**Part 5: Independent Data Set evaluation**  


**Part 6: Test data density effect**  
Do the models predict better in areas of high source data density? 


**Part 7: Making the study area substrate predictions**  

====================================

# Development Notes

Eds first R Markdown document. PDF is easy. Going big by sending it MS Word.

Generates plots from existing data objects in the environment, and code chunks also do some modifying (e.g., scaling of the TSS).  

Next step is to more clearly separate object production, saving, and loading, from figure production. Significant rebuild. While full reproducibility means code chunks should do all the work, not super practical at this point. 

For Knitr to see the existing objects, need to run knitr in the gobal environment (console):   

x <- substr(  Sys.time(), 1, 10)
rmarkdown::render( "substrate_figures.Rmd", 
  output_file = paste0('SubstrateFigures_', x ),
  output_dir = results.dir ) 


Is it best to put captions in the code block, or in the text sections?   
How to set plot dimensions for ggplot()? 
How to set a page to landscape (doesn't seem possible for word).


Details on using R Markdown  <http://rmarkdown.rstudio.com>  
On defining a style to serve as a pagebreak  <https://datascienceplus.com/r-markdown-how-to-insert-page-breaks-in-a-ms-word-document/>  
Guidance for landscape tables/pages <https://code-examples.net/en/q/18a6fd6>  
Messing with figure sizing <https://sebastiansauer.github.io/figure_sizing_knitr/>  
Some info on KNITR and environment <https://stackoverflow.com/questions/34029611/how-to-use-objects-from-global-environment-in-rstudio-markdown>

From the definitive guide <https://bookdown.org/yihui/rmarkdown/>

re figure sizing:
#```{r, fig.width=10, fig.height=2, fig.fullwidth=TRUE}
#par(mar = c(4, 4, .1, .2)); plot(sunspots)
#```
