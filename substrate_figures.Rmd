---
title: "Substrate Tables and Figures"
author: "Edward Gregr"
date: "6/29/2020"
output: word_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 

```
# Overview 

Since the meeting on 2020/06/18:
- colour-blind palette done
    Now hard to find text that works on all heat map colours
    Obs/Pred and Model/Map pairs could have different (but consistent colours)
    
- TSS difference done for depth classes
- TSS scaled on all plots and tables

TO DO:  
- Create new Suppl. materials section.   
- Need decision re density comparison. Necessary? Its conflated with depth. Does this matter? 

Worth doing by Region? How best to structure? (e.g., pick an earlier figure as a template?).

Group interpretation (2020/06/18):   
- Higher dens sampling areas closer to shore, and more heterogeneity close to shore. Hi dens required to capture this hetro, and so these areas better captured  

- High density areas more heterogeneous, this thought to potentially confuse the model, leading to reduced accuracy.   

To Do: Revisit with top/bottom 25%  

Two parts: Density of points has higher imbalance ? but we are not dealing with complexity of the coastline, which we would see, perhaps if we did it by region.   

- Pontius has different names for User/Producer accuracies (commission/omission rates)  
Need to standardize on the best stats terminology (Joanne/Ed to work on this)  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

## Data processing steps

All data structures are saved as .RData files. 

### IDE_Main.R

**PART 1: Load, prep, and inspect all the data.**   
	Includes Obs data w 100 m predictors; All IDS data; Regional 20 m predictors
	
Data structures: *point.data, dive.20mIV, cam.20mIV, ROV.20mIV, obs.100mIV, obs.20mIV*  
Standardized predictor names: *names.100m*  

**PART 2: Build and evaluate the required RF models.**  
	Creates RF model structures, and summarizes Build statistics

Data structures: rf.region.Coast, rf.region.HG, rf.region.NCC, rf.region.WCVI,  
	rf.region.QCS, rf.region.SOG  
Build summaries: *build.results*   

CSV Files: Build_results_Integrated.csv - build.results.Integrated
	   Build_results_byClassStats.csv - build.results.ByClass
	   Build_results_testPrevalence.csv - build.results.ClassPrev
	   Build_results_varImportance.csv - build.results.VarImport


**Part 3: Produce and save heatmaps (4) of build results**  


**Part 4: Model resolution tests**  


**Part 4b: Model resolution across depths**  
Take the Coastwide model, test its performance by depth, in each region.  
Take the regional models, test their performance by depth, in each region.  
(Coastwide is 100m, regional are 20 m. Points are the same)

CSV Files: Stats_byRibbon_byRegion_byModel.csv
	   TSS_byRibbon_byRegion_byModel.csv (for heatmaps)


**Part 5: Independent Data Set evaluation**  


**Part 6: Test data density effect**  
Do the models predict better in areas of high source data density? 


**Part 7: Making the study area substrate predictions**  


====================================

```{r, SetUp, echo=FALSE }
#```{r code = readLines('substrate_functions.R')}

# Load saved RF models ... 
#load( file.path( model.dir, 'rf_allModels_2020-05-21-1019.RData' ))

# Load build statistics and results ... 
#load( file.path( model.dir, 'buildResults_2020-05-21-1019.RData' ))

```


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


# Tables 

Main results table summarizes build statistics.   

```{r Build_Tables, echo=FALSE}
library( knitr )

#-- Main statistics don't make a very nice bar plot. Report as table. 
T.cap <- "Table 1: Summary of model build results." 
kable( build.sum$build.results.Integrated, digits=2, caption = T.cap )


# Shrink font ... 
# Will it be easier to format Caption OUTSIDE w pandoc or INSIDE w kable?
# digits will need to be a vector if you want to use it (cuz N)

# Create table of how 100 m coastal model predicts at regiona scale. 
# REQUIRES test.regions to exist. 
foo <- rbind( 
    cbind( 'Region' = 'Coast', Results.Row( rf.region.Coast, test.regions$Coast )$Integrated), 
    cbind( 'Region' = 'HG', Results.Row( rf.region.Coast,    test.regions$HG )$Integrated), 
    cbind( 'Region' = 'NCC', Results.Row( rf.region.Coast,   test.regions$NCC )$Integrated),
    cbind( 'Region' = 'WVVI', Results.Row( rf.region.Coast,  test.regions$WCVI )$Integrated), 
    cbind( 'Region' = 'QCS', Results.Row( rf.region.Coast,   test.regions$QCS )$Integrated),
    cbind( 'Region' = 'SOG', Results.Row( rf.region.Coast,   test.regions$SOG )$Integrated)
  )
rownames( foo ) <- NULL

T.cap <- "Table 2: Performance of the 100 m model by region." 
kable( foo, digits=2, caption = T.cap )
```   
   
NOTES: Table 2 sample size is different from Table 1 because Obs data are separated using regional shape file, whereas in Table 1 points are selected based on regional, 20m rasters.   


Table 2 shows that 100 m model performance is not skewed towards any particular region. That is, it doesn't fit the witheld data any better in one region over another.  

Reasons for thinking processes not stationary: SOG = muddy, NCC = Fjords, WCVI/HG exposed   

Quantity surprisingly low. Exchange potentially driven by Mixed. Can we understand this by looking at class-specific metrics?  



```{r IDE_Tables, echo=FALSE}
library( knitr )

T.cap <- "Table 3: Performance of each model for each independent data set." 
kable( IDE.Results.Table(), digits=2, caption = T.cap )


T.cap <- "Table 4: Performance of bottom patch models for regions with large IDS samples." 
kable( IDE.BoP, digits=2, caption = T.cap )

# Build ANOVA from depth.results argument ... 

# # Copied from IDE_Main.R ... 
# names(depth.results)
# cor( depth.results[, -c(1:3)] )
# 
# # replace TSS w other metrics to create results. 
# a <- lm( TSS ~ Model + Region + Ribbon + N + Imbalance, data = depth.results )
# summary(a)
# anova( a )

```   

NOTES:  
- Defer building ANOVA table until writing mostly done  




# Figures - Part 1: Summary of model building

## Heatmaps
```{r Build_Heatmaps, echo=FALSE}

#--- Heatmaps (tigures!) of build stats as png files from above csv files.
library( superheat)

# Library only makes pngs. Can call here, then use pandoc to load the png.
# BUT: one of the two calls below plots the palette for some reason ... ?

# Plot.Build.Class.Stats( build.sum$build.results.ByClass, pal.heat.10, 800, 600 )
# Plot.Build.Var.Import( build.sum$build.results.VarImport, pal.heat.11, 1000, 600 )

# Producer accuracy to be re-made with NO x-axis labels ... 
# Then can lose the height= ... which doesn't seem to work? Neither does the fullwidth option :\

```

![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/heat_ProducerAccuracy_Build.png)
![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/heat_UserAccuracy_Build.png)

**Figure 1a/b: Heatmaps of class-statistics (User/Producer) across models**  

```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

![](C:/Users/Edward/Dropbox/DFO Job/Substrate2019/Results/heat_VariableImportance_Build.png){ fig.fullwidth=TRUE }

**Figure 2: Heatmap of variable importance across models**  

Importance is measured as the proportion of each predictor's contribution to each individual model, relative to the predticor with the maximum predictive power. (p/max(p)).  

NA indicates that Fetch was not used as a predictor for the Coast model.  

Also note that while there is a rank order, some of the predictors are very similar, e.g., NC has 4 predictors with virtually equal model contribution scores (0.5 and 0.51).  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r echo=FALSE, fig.width=10, fig.height=6, fig.fullwidth=TRUE}

#-- Prevalence plots (obs & predicted) for build (testing partition), faceted by Region
Plot.Obs.Pred.Prevalence.Build( build.sum$build.results.ClassPrev, pal.cb2 ) 
# save plot to 'a' if using ggsave()
# ggsave("Class Prevalence for Obs&Pred (Build) for Regions.png", a, dpi = 300, width = 16, height = 10, path = results.dir)

```

## Build results
**Figure 3: Comparison of observed and predicted prevalence for the Obs testing partition, by class, across regions.**  


Hard overpredicted in all regions.  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r echo=FALSE, fig.width=10, fig.height=6}
#-- User and producer accuracies by class ... 
Plot.Class.Stats.For.Regions( build.sum$build.results.ByClass, pal.cb2 )
# save plot to 'a' if using ggsave()
# ggsave( 'Class Stats (Build) for Regions.png', a, dpi = 300, width = 16, height = 10, path = results.dir)

```

**Figure 4: User and Producer accuracies for the Obs testing partition, by class, across regions.**


First? Indication that Mixed is a bit of a problem  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r Depth_Results_TSS_by_Resolution, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results2

difTSS <- z[ z$Model == '100 m', ]$TSS - z[ z$Model == '20 m', ]$TSS
z <- cbind( z, 'diffTSS' = difTSS)

#Plot.TSS.By.Depth.For.Regions(  z[ , c('Model', 'Region', 'Ribbon', 'TSS')], pal.cb2 )
Plot.TSS.By.Depth.For.Regions2( z[ , c('Model', 'Region', 'Ribbon', 'diffTSS')], pal.cb2[2] )

#-- vs. What does a tigure look like? Not so good ... 
# out.file <- 'TSS_byRibbon_byRegion_byModel.csv'
# write.csv( depth.results[ , c("Model", "Region", "Ribbon", "TSS")], file = file.path(results.dir, out.file) )
# 
# Plot.Region.Class.Stats( 'TSS_byRibbon_byRegion_byModel.csv', '20 m', pal.10 )
# Plot.Region.Class.Stats( 'TSS_byRibbon_byRegion_byModel.csv', '100 m', pal.10 )

```

**Figure 5: Performance of 20 and 100 m models against the Obs testing partition, across depth ranges, by region.**  

Agreed 2020/06/18 that difference plot was a good idea. 
```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Depth_Results_Obs_Pontius, echo=FALSE, fig.width=10, fig.height=6, out.width='100%'}

z <- depth.results2[ depth.results2$Model == '20 m', ]

Plot.Pontius.By.Depth.For.Regions( z, rev(pal.cb4) )


```

**Figure 6: Performance of 20 m models across depth ribbons, by region (Obs testing partition.**  



## Figures - Part 2: Independent Data Evaluation

```{r IDS_One, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
# Depends on IDE.results created in main script. 
#--- 1) Begin by looking at IDS sample sizes using perClass prevalences 
x <- IDE.results$PerClass
y <- x[ x$Stat == 'PrevObs', ]

# Tidy the data a bit  ... 
y <- x[ x$Stat == 'PrevObs', ] %>% subset(select = -c(Stat))
colnames(y) <- c( 'Region', 'IDS', 'Rock', 'Mixed', 'Sand', 'Mud')
rownames(y) <- NULL

# fill in  blank data to balance plot ... 
y <- rbind( y, data.frame( 'Region' = 'WCVI', 'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0),
               data.frame( 'Region' = 'QCS',  'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0),
               data.frame( 'Region' = 'SOG',  'IDS' = 'ROV', 'Rock'=0, 'Mixed'= 0, 'Sand'=0, 'Mud'=0) )

Plot.Obs.By.IDS.For.Regions( y, pal.cb3, sz = 25, lx=0.83, ly=0.75 )

```

**Figure 6: Prevalence of Independent data sets by depth class, across regions. **  

Agreed this will go in Supplemental Materials   


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r IDS_Two, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}

# 3) Look at Integrated statistics for each IDS by Region.
# Use a predetermined collection of stats (see function)

# Again, start with a little data prep ... 

y <- IDE.results$Integrated
z <- y[ y$Region %in% c('Coast', 'HG', 'NCC'), ]
rownames(z) <- NULL

# This function can be used to generate various views of the data. 
# 2020/05/25 DH preferred IDS as the grouping variable ... 
a <- Plot.Stats.By.IDS.For.Regions( z, pal.cb3, sz = 25, lx=0.76, ly=0.87 )
a

# Saving hi res image for manuscript ... 
#ggsave( paste0( 'Figure 8 - Integrated Stats by IDE for Regions.png'), a, dpi = 300, width = 16, height = 10, path = results.dir)
```

**Figure 7: Integrated statistics for Independent data sets by Region.**  

The TSS for this figure, nominally on the scale [-1, +1], is re-scaled to for better comparison with the other statistics which range from [0, 1]. Reported value is (TSS+1)/2 to scale onto [0, 1].

Cam/ROV better at predicting absence; but why? Look at producer/user accuracies.  

ROV takes dominant class within each pixel.   

Drop cam performance related to depth problem. Lat/lon not accurate because of deflection of camera from vessel.  ROV data corrected to some degree. Sampling bias.  

Include some guidance on drop camera (and other?) data collection. Technical solutions, as well as sample design questions (e.g., how to generate an independent set fo effectively evaluate; e.g., can investigate the heterogeneity in a location. Point is that just cuz something is IDS doesn?t it is ?clean?.   

Also, the close-to-shore challenge of matching the recorded depth to the bathymetric resolution.  Predictor resolution.  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r IDS_Three, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}

# What about TSS vs. Quantity and Allocation?
# Remember: Accuracy = 1 – (Quantity + Exchange + Shift).

y <- IDE.results$Integrated
z <- y[ y$Region %in% c('Coast', 'HG', 'NCC'), ]

z$Allocation <- z$Shift + z$Exchange
z <- z[, c( 'Region', 'IDS', 'Accuracy', 'Quantity', 'Exchange','Shift' )]

Plot.Pontius.By.IDS.For.Regions( z, rev(pal.cb4), sz = 25 )
```

**Figure 8: Pontius statistics for Independent data sets by region.**  


As with 7, produce another that is by depth classes.   


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```


```{r IDS_By_Ribbon_A, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
# Pontius stats (Accuracy, Quantity, Exchange, Shift) for IDS, by ribbon, 
# one plot for each region.
# Remember: Accuracy = 1 – (Quantity + Exchange + Shift).

z <- IDE.depths[ IDE.depths$Region == 'Coast', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]
Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  
**Figure 9a: Pontius statistics for Independent data sets by depth ribbon - Coastwide.**  


```{r IDS_By_Ribbon_B, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
z <- IDE.depths[ IDE.depths$Region == 'HG', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]
Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  
**Figure 9b: Pontius statistics for Independent data sets by depth ribbon - Haida Gwaii.**  

```{r IDS_By_Ribbon_C, echo=FALSE, out.width='120%', fig.fullwidth=TRUE}
z <- IDE.depths[ IDE.depths$Region == 'NCC', c( 'IDS', 'Ribbon', 'Accuracy', 'Shift','Exchange','Quantity' )]
Plot.Pontius.By.IDS.Depth.For.Regions( z, rev(pal.cb4), sz = 25 )
```  
**Figure 9c: Pontius statistics for Independent data sets by depth ribbon - North Central Coast.**  


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

```{r Prevalences, echo=FALSE, fig.fullwidth=TRUE }
#-- Plots the Study Area prevalence results. Requires the map.prev structure. 
Plot.Pred.Map.Prevalence( map.prev, build.sum, pal.RMSM)


```

**Figure 10: Comparison of prevalence in predictions between fitted model (points) and study area (Map).**    


Mapped prevalences are a reasonable reflection of true prevalences by region  

Appeal to author experience. And also technical argument why this would be so. e.g., stationarity across process; we know there is sample bias, would be disturbing to see that reflected in the predicted surface. Shows model is working! And that RF method is dealing well with prevalence.   

Include illustration of value of wtd RF. Good conclusion, emphasizing the importance of dealing with prevalence in models. Table/Fig for suppl. Materials.  

Standardize terminology with earlier figure that compares build/test prevalences   


```{=openxml}
# Inserting a word page break ... 
<w:p>
  </w:pPr>
    <w:r>
      <w:br w:type="page"/>
    </w:r>
</w:p>
```

# Development Notes

Eds first R Markdown document. PDF is easy. Going big by sending it MS Word.

Generates plots from existing data objects in the environment, and code chunks also do some modifying (e.g., scaling of the TSS).  

Next step is to more clearly separate object production, saving, and loading, from figure production. Significant rebuild. While full reproducibility means code chunks should do all the work, not super practical at this point. 

For Knitr to see the existing objects, need to run knitr in the gobal environment:   
  Run rmarkdown::render("substrate_figures.Rmd") from console.


Is it best to put captions in the code block, or in the text sections?   
How to set plot dimensions for ggplot()? 
How to set a page to landscape (doesn't seem possible for word).


Details on using R Markdown  <http://rmarkdown.rstudio.com>  
On defining a style to serve as a pagebreak  <https://datascienceplus.com/r-markdown-how-to-insert-page-breaks-in-a-ms-word-document/>  
Guidance for landscape tables/pages <https://code-examples.net/en/q/18a6fd6>  
Messing with figure sizing <https://sebastiansauer.github.io/figure_sizing_knitr/>  
Some info on KNITR and environment <https://stackoverflow.com/questions/34029611/how-to-use-objects-from-global-environment-in-rstudio-markdown>

From the definitive guide <https://bookdown.org/yihui/rmarkdown/>

re figure sizing:
#```{r, fig.width=10, fig.height=2, fig.fullwidth=TRUE}
#par(mar = c(4, 4, .1, .2)); plot(sunspots)
#```
